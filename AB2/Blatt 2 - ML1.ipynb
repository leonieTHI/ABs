{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # \"Header\"\n",
    "\n",
    "<p style=\"text-align: left;  font-size:18pt; LINE-HEIGHT:30px;\">\n",
    "    <span style=\"float: left\">\n",
    "     Technische Hochschule Ingolstadt<br>\n",
    "     Prof. Dr. Sören Gröttrup\n",
    "    </span>\n",
    "    <span style=\"float: right;\">\n",
    "       Machine Learning 1<br>\n",
    "        <span style=\"float: right;\">WS 23/24</span>\n",
    "    </span>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[//]: # \"Header Aufgabenblatt\"\n",
    "<br>\n",
    "<p style=\"text-align: center;  font-size:18pt; LINE-HEIGHT:30px;\">\n",
    "     <span style=\"font-weight: bold;\">Aufgabenblatt 2</span><br>\n",
    "     Themen: Polynomiale Regression, Kostenfunktion, Gradientenabstiegsverfahren<br>\n",
    "     Abgabetermin: 09.11.2023, 23:59 Uhr<br>\n",
    "     Punkte: 25\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Namen:** _Bitte tragen Sie hier die Namen der Abgabegruppe ein._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Polynomiale Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten\n",
    "Der Datensatz `pressure_train.csv` (auf der Moodle-Seite des Praktikums (https://moodle.thi.de/course/view.php?id=6824)) enthält gemessene Werte der Temperatur (in Celsius) und des Dampfdrucks von Quecksilber. Plottet man die beiben Variablen sieht man, dass kein linearer Zusammenhang besteht.\n",
    "\n",
    "![temp_pressur](Bilder/temp_pressure.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "Der Root Mean Squared Error ist ein Standardmaß zur validierung der Güte eines Regressionsmodells. Seien $y^{(1)},...,y^{(n)}$ die wahren Werte und $f(x^{(1)}),...,f(x^{(n)})$ die Vorhersagen der Regression, dann ist der RMSE gegeben durch\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n \\left(y^{(i)} - f(x^{(i)})\\right)^2}.$$\n",
    "\n",
    "Der RMSE ist also nicht anderes als die Wurzel aus den mittleren quadratischen Fehlern, oder die Wurzel aus der quadratischen Kostenfunktion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 1.1 [8 Punkte]\n",
    "In dieser Aufgabe sollen Sie mittels **polynomialer Regression** auf den Daten aus dem Datensatz `pressure_train.csv` den Zusammenhang zwischen Dampfdruck aus der Temperatur modellieren.\n",
    "1. Führen Sie explorative Analysen auf dem Datensatz `pressure_train.csv` durch (z.B. Korrelationsanalysen oder grafische Darstellungen), um zu entscheiden welchen Grad der polynomialen Regression Sie verwenden wollen. Begründen Sie ihre Wahl. D.h., wollen Sie ein Polynom mit dem Grad 2,3,4 oder höher an die Daten anpassen?\n",
    "1. Erstellen Sie ein plonymielles Regressionsmodell `poly_reg` mit dem in 1.) ermittelten Grad.\n",
    "1. Wie groß ist der **Root Mean Squared Error (RMSE)** auf den Daten `pressure_train.csv`? _Hinweis_: Verwenden Sie die Funktion `from sklearn.metrics import mean_squared_error`.\n",
    "1. Importieren Sie den Testdatensatz `pressure_teset.csv`. Wie groß ist der Root Mean Squared Error (RMSE) auf den Testdaten? _Benchmark für den RMSE auf den Testdaten ist: 5.5_. Versuchen Sie ein Model mit einem besseren RMSE zu erstellen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lösung Aufgabe 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teil 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lösung der Aufgabe 1.1\n",
    "\n",
    "#### Code here ####\n",
    "\n",
    "# Teil 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antwort:** ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teil 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Code here ####\n",
    "\n",
    "# Teil 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teil 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Code here ####\n",
    "\n",
    "# Teil 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teil 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Code here ####\n",
    "\n",
    "# Teil 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Gradientenabstiegsverfahren für die logistische Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei einer logistischen Regression wird die Wahrscheinlichkeit für die Zugehörigkeit zur Klaase 1 prognostiziert durch:\n",
    "$$p(1|x) = \\frac{1}{1+\\exp(-f(x))}$$\n",
    "mit \n",
    "$$f(x) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_m x_m.$$\n",
    "Die Standard-Kostenfunktion zur Bewertung der Modellgüte ist die **Cross-Entropy**\n",
    "$$C(\\beta)=-\\frac1n\\sum_{i=1}^n y^{(i)}\\ln(p(1|x))+(1-y^{(i)})\\ln(1-p(1|x)),$$\n",
    "wobei $n$ die Größe des Trainingsdatensatzes beschreibt und $y^{(i)}\\in\\{0,1\\}$.\n",
    "\n",
    "Ein iteratives Verfahren zur Ermittlung der optimalen Parameter $\\beta_0,...,\\beta_m$ ist das Gradientenabstiegsverfahren. In jedem Iterationsschritt werden die Parameter dabei nach der folgenden Vorschrift so lange angepasst, bis sich Konvergenz einstellt oder die maximale Anzahl an Iterationen erreicht ist\n",
    "$$\\beta = \\beta - \\alpha \\nabla C(\\beta)$$\n",
    "bzw. für jedes $j=0,...,m$\n",
    "$$\\beta_j = \\beta_j - \\alpha \\frac{d}{d\\beta_j} C(\\beta).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.1 (Berechnung des Gradienten) [4 Punkte]\n",
    "Berechnen Sie den Gradienten $\\nabla C(\\beta)$ für die quadratische Kostenfunktion.\n",
    "\n",
    "_Hinweis_: Benutzen Sie die Kettenregel zur Bestimmung der Ableitungen. D.h. bestimmen Sie nacheinander die Ableitungen\n",
    "* $\\frac{d}{d p} L(y,p)$ für $L(y,p)=y^{(i)}\\ln(p)+(1-y^{(i)})\\ln(1-p)$ (Cross-Entropy)\n",
    "* $\\frac{d}{d z} S(z)$ für $S(z)=\\frac{1}{1+\\exp(-z)}$ (logistische Sigmoid-Funktion)\n",
    "* $\\frac{d}{d \\beta_i} f(x)$ für $f(x) = \\beta_0 + \\beta_1 x_1 + ... + \\beta_m x_m$ (Regression)\n",
    "\n",
    "Die Kostenfunktion ist dann nämlich\n",
    "$$ C(\\beta) = L\\left(y,S(f(x)) \\right) $$\n",
    "Bestimmen Sie dann über die Kettenregel die Ableitung der Kostenfunktion $\\frac{d}{d\\beta_i} C(\\beta)$\n",
    "\n",
    "\n",
    "1. Welche der folgenden Terme ist die korrekte Ableitung für $\\frac{d}{d\\beta_0} C(\\beta)$?\n",
    "    1. $\\frac{d}{d\\beta_0} C(\\beta) =  1$\n",
    "    1. $\\frac{d}{d\\beta_0} C(\\beta) = -\\sum_{i=1}^n \\left(y^{(i)}-p(1|x^{(i)}) \\right)$\n",
    "    1. $\\frac{d}{d\\beta_0} C(\\beta) = \\frac{1}{n}\\sum_{i=1}^n \\left(y^{(i)}-p(1|x^{(i)}) \\right)$\n",
    "    1. $\\frac{d}{d\\beta_0} C(\\beta) = -\\frac{1}{n}\\sum_{i=1}^n \\left(y^{(i)}-p(1|x^{(i)}) \\right)$\n",
    "   \n",
    "1. Welche der folgenden Terme ist die korrekte Ableitung für $\\frac{d}{d\\beta_j} C(\\beta)$ für $j>0$?\n",
    "    1. $\\frac{d}{d\\beta_j} C(\\beta) = -\\frac{1}{n}\\sum_{i=1}^n x^{(i)}_j\\left(y^{(i)}-p(1|x^{(i)}) \\right)$   \n",
    "    1. $\\frac{d}{d\\beta_j} C(\\beta) = \\sum_{i=1}^n \\left(y^{(i)}-p(1|x^{(i)}) \\right)$\n",
    "    1. $\\frac{d}{d\\beta_j} C(\\beta) = -\\frac{1}{2n}\\sum_{i=1}^n x^{(i)}_j\\left(y^{(i)}-p(1|x^{(i)}) \\right)$\n",
    "    1. $\\frac{d}{d\\beta_j} C(\\beta) = \\frac{1}{n}\\sum_{i=1}^n x^{(i)}_j\\left(y^{(i)}-p(1|x^{(i)}) \\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antwort zu Aufgabe 2.1** <br>\n",
    "1. ...\n",
    "1. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.2 (Zusatzaufgabe) (One-Hot-Encoding) [4 Punkte*]\n",
    "**Diese Punkte sind nicht relevant für die Bestimmung der Bestehensgrenze, können aber von Ihnen als Zusatzpunkte gesammelt werden.**\n",
    "\n",
    "Auf der Moodle Seite des Praktikum (https://moodle.thi.de/course/view.php?id=6824) finden Sie den Datensatz `insurance.csv`, welcher zu medizinischen Informationen eines Patiente auch die Kosten für die Krankenversicherung (`charges`) und die logarithmierten Kosten der Krankenversicherung (`log_charges`) enthält.\n",
    "\n",
    "Im Datensatz befinden sich auch die drei kategoriellen Variablen `sex, smoker, region`. Bevor ein lineares Regressionsmodell in Python auf Daten trainiert werden kann, müssen alle kategoriellen Variablen in numerische transformiert werden.\n",
    "\n",
    "Ihre Aufgabe: Führen Sie ein One-Hot-Encoding für die obigen drei Variablen durch und generieren Sie so einen neuen Datensatz `data_enc`, welcherr ausschließlich aus numerischen Variablen besteht.\n",
    "\n",
    "_Hinweis_: Python stellt bereits eine Funktion für das One-Hot-Encoding bereit (`from sklearn.preprocessing import OneHotEncoder`) mittels der ein Encoding einfach durchgeführt werden kann.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lösung für Aufgabe 2.2\n",
    "\n",
    "#### Code here ####\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2.3 (Implementierung des Gradientenverfahrens) [10 Punkte]\n",
    "In dieser Aufgabe sollen Sie das Gradientenabstiegsverfahren für die lineare Regression implementieren und ein lineares Regressionsmodell auf dem encodeten Datensatz aus der vorherigen Aufgabe erstellen, welches aus den medizinischen Informationen die logarithmischen Kosten `log_charges` prognostiziert. Den Datensatz mit den kodierten Variablen finden Sie auch auf der Moodle Seite des Praktikums (https://moodle.thi.de/course/view.php?id=6824) unter dem Namen `insurance_encoded.csv`.\n",
    "\n",
    "_Hinweis_: Anstatt bei der Implementierung Schleife zu verwenden, können Sie auch die schnelleren Matrix-Operationen benutzen. Die Matrixmultiplikation kann z.B. durch die Funktion `numpy.dot` durchgeführt werden. Auch wichtig könnte die Funktion `numpy.transpose` zum transponieren einer Matrix sein.\n",
    "\n",
    "Während des Gradientenabstiegsverfahren (GAV) werden mehrere Schritte ausgeführt, die in einzelnen Funktionen implementiert werden müssen.\n",
    "1. **Schritt (Initialisierung der Parameter):** Zum Start des GAV müssen die Parameter initialisiert werden. Dies soll durch die Funktion `initialize_param(size_data)` erfolgen, die nach Eingabe der Datensatzgröße `size_data` einen Vektor der Länge `size_data` bestehend aus 1sen ausgibt. Ergänzen Sie den fehlenden Code in der Funktion `initialize_param(size_data)`.\n",
    "\n",
    "In jedem Iterationsschritt werden jetzt die folgenden Schritte nacheinander durchgeführt:\n",
    "\n",
    "2. **Schritt (Vorhersage der Werte der Zielvariablen):** Als erstes muss die Zielvariable für das aktuelle Modell (die aktuellen Parameter) berechnet werden. Die Funktion `predict(X, param)` berechnet für die Feature-Matrix `X` und die aktuellen Parameter `param` den Output der Regression. Ergänzen Sie den fehlenden Code in der Funktion `predict(X, param)`. _Hinweis_: Nutzen Sie Matrix-Operationen.\n",
    "\n",
    "2. **Schritt (Berechnung der Gradienten der Kostenfunktion):** Basierend auf der Vorhersage aus dem vorherigen Schritt können jetzt die Gradienten der Kostenfunktion berechnet werden. Die Funktion `calculate_gradient_l2(Y, X, pred)` berechnet für die Eingabe der Feature-Matrix `X`, der realen Zielwerte `Y` und der Prognose durch das Modell `pred` den Wert der Gradienten für die quadratische Kostenfunktion. Ergänzen Sie den fehlenden Code in der Funktion `calculate_gradient_l2(Y, X, pred)` zur Berechnung der Gradienten. _Hinweis_: Nutzen Sie Matrix-Operationen.\n",
    "\n",
    "2. **Schritt (Update der Parameter):** Im letzten Schritt werden die Parameter mittels Substraktion der Gradienten (siehe obige Formel) angepasst. Die Funktion `update_param(param, grad, learnrate)` führt nach Eingabe der Parameter `param`, der Gradienten `grad` und der Lernrate `learnrate` diese Operation durch und gibt die neuen Parameter zurück. Ergänzen Sie den fehlenden Code in der Funktion `update_param(param, grad, learnrate)`.\n",
    "\n",
    "Die Funktion `gradient_descent_lin_reg(X, Y, learning_rate, max_iter)` führt das GAV durch, indem die obigen Schritte nacheinander durchgeführt werden.\n",
    "\n",
    "5. Implemenieren Sie das GAV indem Sie die obigen Schritte in der richtigen Reihenfolge in der Funktion `gradient_descent_lin_reg(X, Y, learning_rate, max_iter)` ergänzen. Speichern Sie die neuen Parameter in der Variable `param`, welche von der Funktion zurückgegeben wird.\n",
    "\n",
    "6. Generieren Sie aus dem Insurance-Datensatz (`insurance_encoded.csv`) die Zielvariable `Y`, welche die logarithmierten Kosten `log_charges` enthält und eine geeignete Feature-Matrix `X` (Achtung: Sind alle restlichen Variablen als Inputs geeignet?). Fügen Sie zu `X` auch eine weitere Spalte `bias` hinzu, welche nur 1sen beinhaltet und die Konstante in der linearen Gleichung repräsentiert. Trainieren Sie ein lineares Regressionsmodell mittels des GAV für die Daten indem Sie den folgenden Code ausführen:\n",
    "\n",
    "```python\n",
    "costs, param = gradient_descent_lin_reg(X, Y, learning_rate=0.0001, max_iter=40, err=1e-3)\n",
    "```\n",
    "\n",
    "Als Evaluation des Trainingsprozesses und des Models führen Sie folgenden Code aus\n",
    "\n",
    "```python\n",
    "plt.plot(costs)\n",
    "plt.title(\"Costs\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(predict(X, param)-Y)\n",
    "plt.title(\"Distribution of errors\")\n",
    "plt.show()\n",
    "\n",
    "# RMSE\n",
    "print(\"RMSE: \", np.sqrt(np.mean((predict(X, param)-Y)**2)))\n",
    "```\n",
    "\n",
    "Zur Kontrolle: Sie sollten sehen, dass die Kostenfunktion streng und schnell fällt. Nach 40 Iterationen sollte sie einen Wert von ca. 1.63 aufweisen. Der RMSE liegt ungefähr bei 1.8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laden der pakete und Hilfsfunktion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate loss\n",
    "def calculate_L2_cost(Y, pred):\n",
    "    l2_loss = 0.5*np.mean((Y-pred)**2)\n",
    "    return(l2_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beginn Ihrer Lösung für Aufgabe 2.3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize parameter\n",
    "def initialize_param(size_data):\n",
    "    \n",
    "    ### --- Code here --- ###\n",
    "    param = ...\n",
    "    ### ----------------- ###\n",
    "    \n",
    "    return(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Forward Function\n",
    "def predict(X, param):\n",
    "    \n",
    "    ### --- Code here --- ###\n",
    "    pred = ...\n",
    "    ### ----------------- ###\n",
    "    \n",
    "    return(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate gradient\n",
    "def calculate_gradient_l2(Y, X, pred):\n",
    "    \n",
    "    ### --- Code here --- ###\n",
    "    grad = ...\n",
    "    ### ----------------- ###\n",
    "    \n",
    "    return(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## update param\n",
    "def update_param(param, grad, learnrate):\n",
    "    \n",
    "    ### --- Code here --- ###\n",
    "    param = ...\n",
    "    ### ----------------- ###\n",
    "    \n",
    "    return (param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## main function for gradient descent\n",
    "def gradient_descent_lin_reg(X, Y, learning_rate, max_iter, err):\n",
    "    \n",
    "    cost_hist = []\n",
    "    \n",
    "    size_data = X.shape[1]\n",
    "    \n",
    "    # initialize parameter vector\n",
    "    param = initialize_param(size_data)\n",
    "\n",
    "    # Gradient descent iterations\n",
    "    for i in range(max_iter):\n",
    "        \n",
    "        ### --- Code here --- ###\n",
    "        \n",
    "        prediction = ...\n",
    "        grad = ...\n",
    "        param = ...\n",
    "\n",
    "        ### ----------------- ###\n",
    "        \n",
    "        current_cost = calculate_L2_cost(Y, prediction) \n",
    "        cost_hist.append(current_cost)\n",
    "        print(f'Iteration: {i} cost: {current_cost:10.8f}')\n",
    "        \n",
    "        # Stop if convergence is ensured\n",
    "        if np.mean(grad**2)<err:\n",
    "            break\n",
    "        \n",
    "    return cost_hist, param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lösung zu Aufgabenteil 6. (Model training)\n",
    "\n",
    "#### Code here ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run gradient descnet\n",
    "costs, param = gradient_descent_lin_reg(X, Y, learning_rate=0.0001, max_iter=40, err=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation of Training and Modell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validation of Model and \n",
    "plt.plot(costs)\n",
    "plt.title(\"Costs\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(predict(X, param)-Y)\n",
    "plt.title(\"Distribution of errors\")\n",
    "plt.show()\n",
    "\n",
    "# RMSE\n",
    "print(\"RMSE: \", np.sqrt(np.mean((predict(X, param)-Y)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Kostenfunktion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3.1 [3 Punkte]\n",
    "Die 4 unteren Bilder zeichen jeweils eine Decision Boundary einer binäre Klassifikation zwischen Kreisen und Quadraten. Die roten Kreise sind neue hinzugenommene Daten für die jeweils die Werte einer der folgenden Versultfunktione angegeben sind **0-1-Loss, Hinge-Loss, Logistic-Loss** oder **Exponential-Loss**. Ordnen Sie die Bilder der jeweils verwendeten Verlustfunktion zu.\n",
    "\n",
    "**A**\n",
    "\n",
    "<img src=\"Bilder/loss_function1.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "**B**\n",
    "\n",
    "<img src=\"Bilder/loss_function 31.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "**C**\n",
    "\n",
    "<img src=\"Bilder/loss_function 2.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "**D**\n",
    "\n",
    "<img src=\"Bilder/loss_function 4.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Antwort für Aufgabe 3.1**\n",
    "* 0-1-Loss = ...\n",
    "* Hinge-Loss = ...\n",
    "* Logistic-Loss = ...\n",
    "* Exponential-Loss = ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
